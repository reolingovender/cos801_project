{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "d9f7_bpZBpx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we insatll the neccessary packahe versions"
      ],
      "metadata": {
        "id": "P2p4Ytp1uvk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"tensorflow-text==2.17.*\""
      ],
      "metadata": {
        "id": "Yc62spd0FcKL",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7723ad42-1789-4f66-83f1-aef044c57c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-text==2.17.*\n",
            "  Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.17.*) (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text==2.17.*) (0.1.2)\n",
            "Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tf-models-official==2.17.*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B2bQgQ8Sb_M6",
        "outputId": "bd046929-0c37-46ac-91e5-bc5e25306f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-models-official==2.17.*\n",
            "  Downloading tf_models_official-2.17.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (3.0.11)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (11.0.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (2.151.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (4.2.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (1.6.17)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (1.26.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (4.10.0.84)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (2.0.8)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (6.0.2)\n",
            "Collecting sacrebleu (from tf-models-official==2.17.*)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (0.2.0)\n",
            "Collecting seqeval (from tf-models-official==2.17.*)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (4.9.7)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (0.16.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.17.*)\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: tensorflow-text~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (2.17.0)\n",
            "Requirement already satisfied: tensorflow~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (2.17.1)\n",
            "Requirement already satisfied: tf-keras>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (2.17.0)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official==2.17.*) (1.1.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.17.*) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.17.*) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.17.*) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.17.*) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official==2.17.*) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official==2.17.*) (6.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.17.*) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official==2.17.*) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.17.0->tf-models-official==2.17.*) (0.37.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.17.*) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.17.*) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.17.*) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.17.*) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.17.*) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official==2.17.*) (3.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.17.*) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.17.*) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official==2.17.*) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official==2.17.*)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.17.*) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.17.*) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->tf-models-official==2.17.*)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official==2.17.*) (5.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official==2.17.*) (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (8.1.7)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (2.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (1.13.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official==2.17.*) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.17.*) (1.10.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (0.45.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.17.*) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.17.*) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->tf-models-official==2.17.*) (3.21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.17.*) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.17.*) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.17.*) (5.5.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.17.*) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official==2.17.*) (3.10)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.17.*) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.17.*) (3.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official==2.17.*) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official==2.17.*) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official==2.17.*) (3.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.17.*) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.17.*) (1.3)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->tf-models-official==2.17.*) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow~=2.17.0->tf-models-official==2.17.*) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow~=2.17.0->tf-models-official==2.17.*) (0.1.2)\n",
            "Downloading tf_models_official-2.17.0-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=d97eec0a3881e1903758f5094cdc613303b9ba6d69d4ca216c7f97257af82706\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorflow-model-optimization, portalocker, colorama, sacrebleu, seqeval, tf-models-official\n",
            "Successfully installed colorama-0.4.6 portalocker-3.0.0 sacrebleu-2.4.3 seqeval-1.2.2 tensorflow-model-optimization-0.8.0 tf-models-official-2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "lieRWnmkFt5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "XwDo4IH4cAMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GqKJYL0NSie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version and gpu check"
      ],
      "metadata": {
        "id": "nUQm54D6F1dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure gpu is available for processing and that TensorFlow is of the right version"
      ],
      "metadata": {
        "id": "ft8Mvcu4u_YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbYgtpn6CFKA",
        "outputId": "795daebc-b595-4f1c-f0d3-ccc510a051c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm5-SdLDyUJ2",
        "outputId": "d4da96a2-b51c-4d56-d587-101127577f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2tw1Xzdx4DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function definitions"
      ],
      "metadata": {
        "id": "_Bb46yuIF63r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read .txt files and split into paragraphs.\n",
        "def get_split_text(input_file):\n",
        "  f = open(input_file, \"r\")\n",
        "  split = f.read().split('\\n')\n",
        "\n",
        "  return split"
      ],
      "metadata": {
        "id": "Mkt5BQbsGGA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode paragraphs with applicable encoder model\n",
        "def encoding(text, pre_proc, enc):\n",
        "\n",
        "  text_preprocessed = pre_proc(text)\n",
        "  text_encoded = enc(text_preprocessed)\n",
        "\n",
        "  return text_encoded"
      ],
      "metadata": {
        "id": "SvtnTNvpGF2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing"
      ],
      "metadata": {
        "id": "MG-AYzwkGFso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that input paragraphs correspond with labels provided"
      ],
      "metadata": {
        "id": "7I_OPBbXvjDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "C0F2DDPVGFkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train\"\n",
        "dir_list = os.listdir(path)\n",
        "num_all_train_files = int(len(dir_list)/2)\n",
        "num_all_train_files"
      ],
      "metadata": {
        "id": "TqNjEIZxGFb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for errors X\n",
        "l_x = []\n",
        "for F in tqdm(range(1,num_all_train_files+1)):\n",
        "#for F in tqdm(range(1,1000+1)):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "\n",
        "  l_x.append(len(split))"
      ],
      "metadata": {
        "id": "0fXzuct-GFTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_x_rectified = [i-1 for i in l_x]"
      ],
      "metadata": {
        "id": "cN6zKZ5XGFDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for errors y\n",
        "#for F in tqdm(range(1,1000+1)):\n",
        "l_y = []\n",
        "\n",
        "for F in tqdm(range(1,num_all_train_files+1)):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train/truth-problem-'+str(F)+'.json'\n",
        "  with open(url, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    l_y.append(len(data['changes']))"
      ],
      "metadata": {
        "id": "IpnR7hcBFmLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get erroneous indices\n",
        "bad_index = [i for i in range(len(l_x)) if l_x_rectified[i] != l_y[i]]\n",
        "bad_index"
      ],
      "metadata": {
        "id": "ztSYxKlKv02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bad_index = [703, 3385]"
      ],
      "metadata": {
        "id": "-bTjGDJDLeYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove erroneous indices\n",
        "total_range = list(range(1,num_all_train_files+1))\n",
        "print(len(total_range))\n",
        "for i in bad_index:\n",
        "  total_range.remove(i+1)\n",
        "print(len(total_range))"
      ],
      "metadata": {
        "id": "EX9fU9n6zE2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "KpkFpOquv0-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation\"\n",
        "dir_list = os.listdir(path)\n",
        "num_all_test_files = int(len(dir_list)/2)\n",
        "num_all_test_files"
      ],
      "metadata": {
        "id": "ASrLC64QzHl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for errors X\n",
        "l_x_test = []\n",
        "for F in tqdm(range(1,num_all_test_files+1)):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "\n",
        "  l_x_test.append(len(split))"
      ],
      "metadata": {
        "id": "WgUhWh_Zv1H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_x_test_rectified = [i-1 for i in l_x_test]"
      ],
      "metadata": {
        "id": "EYefdlBOwO26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for errors y\n",
        "l_y_test = []\n",
        "\n",
        "for F in tqdm(range(1,num_all_test_files+1)):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation/truth-problem-'+str(F)+'.json'\n",
        "  with open(url, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    l_y_test.append(len(data['changes']))"
      ],
      "metadata": {
        "id": "NWog7OmEwO7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get erroneous indices\n",
        "bad_index = [i for i in range(len(l_x_test)) if l_x_test_rectified[i] != l_y_test[i]]\n",
        "bad_index"
      ],
      "metadata": {
        "id": "fcPadIskwPAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bad_index = []"
      ],
      "metadata": {
        "id": "F5rhkb5zMMv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove erroneous indices\n",
        "total_range_test = list(range(1,num_all_test_files+1))\n",
        "print(len(total_range_test))\n",
        "for i in bad_index:\n",
        "  total_range_test.remove(i+1)\n",
        "print(len(total_range_test))"
      ],
      "metadata": {
        "id": "EDxsXiG7wPGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create X_train and y_train"
      ],
      "metadata": {
        "id": "5cJ0WIQCwPL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pre-processor and encoder model\n",
        "# Encode every paragraph in input file and create difference vector and store in a usable format for TensorFlow\n",
        "# Format labels for use with TensorFlow"
      ],
      "metadata": {
        "id": "P4h86T8KvvmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Small BERT - small_bert/bert_en_uncased_L-4_H-512_A-8"
      ],
      "metadata": {
        "id": "Nn-GIbv8wPXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_bert_pre_process = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
      ],
      "metadata": {
        "id": "LlcC8K_Mv1Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_bert_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'"
      ],
      "metadata": {
        "id": "9Y5fCtgXv1Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_bert_preprocess_model = hub.KerasLayer(small_bert_pre_process)\n",
        "small_bert_model = hub.KerasLayer(small_bert_encoder)"
      ],
      "metadata": {
        "id": "i_xDNP_dCQZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_small_bert = []\n",
        "for F in tqdm(total_range):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "  text_encoded = encoding(split,small_bert_preprocess_model,small_bert_model)\n",
        "  pooled = text_encoded['pooled_output']\n",
        "  for i in range(1,len(pooled)):\n",
        "    X_train_small_bert.append(pooled[i]-pooled[i-1])"
      ],
      "metadata": {
        "id": "fj-_dRtXGBsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npX_train_small_bert = np.array(X_train_small_bert)"
      ],
      "metadata": {
        "id": "XoSkr_Q9IlTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npX_train_small_bert)"
      ],
      "metadata": {
        "id": "kKADkt3vIlYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = []\n",
        "for F in tqdm(total_range):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train/truth-problem-'+str(F)+'.json'\n",
        "  with open(url, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    y_train.extend(data['changes'])"
      ],
      "metadata": {
        "id": "fPm5iIbjIlct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npy_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "BbYq8435Ilgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npy_train)"
      ],
      "metadata": {
        "id": "hgAkktcrIlkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALBERT"
      ],
      "metadata": {
        "id": "6cE0syrGIlnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "albert_pre_process = 'https://tfhub.dev/tensorflow/albert_en_preprocess/3'"
      ],
      "metadata": {
        "id": "fBVF8CdMKkt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "albert_encoder = 'https://tfhub.dev/tensorflow/albert_en_base/2'"
      ],
      "metadata": {
        "id": "krar-1NvKkyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "albert_preprocess_model = hub.KerasLayer(albert_pre_process)\n",
        "albert_model = hub.KerasLayer(albert_encoder)"
      ],
      "metadata": {
        "id": "eJbcI-J6C_La",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_albert = []\n",
        "for F in tqdm(total_range):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "  text_encoded = encoding(split,albert_preprocess_model,albert_model)\n",
        "  pooled = text_encoded['pooled_output']\n",
        "  for i in range(1,len(pooled)):\n",
        "    X_train_albert.append(pooled[i]-pooled[i-1])"
      ],
      "metadata": {
        "id": "KsKiuRgGKk17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npX_train_albert = np.array(X_train_albert)"
      ],
      "metadata": {
        "id": "nlTZv-RHKk5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npX_train_albert)"
      ],
      "metadata": {
        "id": "CX0cRjbGN-fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npy_train)"
      ],
      "metadata": {
        "id": "ffCzbhQpKk9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small Electra"
      ],
      "metadata": {
        "id": "rcNGR3Z8KlD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_electra_pre_process = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
      ],
      "metadata": {
        "id": "hfFq9apSKlHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_electra_encoder = 'https://tfhub.dev/google/electra_small/2'"
      ],
      "metadata": {
        "id": "sRQBaJZKKlLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_electra_preprocess_model = hub.KerasLayer(small_electra_pre_process)\n",
        "small_electra_model = hub.KerasLayer(small_electra_encoder)"
      ],
      "metadata": {
        "id": "OuL1cqb2DGfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_electra = []\n",
        "for F in tqdm(total_range):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-train/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "  text_encoded = encoding(split,small_electra_preprocess_model,small_electra_model)\n",
        "  pooled = text_encoded['pooled_output']\n",
        "  for i in range(1,len(pooled)):\n",
        "    X_train_electra.append(pooled[i]-pooled[i-1])"
      ],
      "metadata": {
        "id": "fVuSpoY7KlOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npX_train_electra = np.array(X_train_electra)"
      ],
      "metadata": {
        "id": "3xPvS3lxKlV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npX_train_electra)"
      ],
      "metadata": {
        "id": "itrbypCkKlcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npy_train)"
      ],
      "metadata": {
        "id": "_yONZev0KliS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create X_test and y_test"
      ],
      "metadata": {
        "id": "tCCRzUpgIlu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pre-processor and encoder model\n",
        "# Encode every paragraph in input file and create difference vector and store in a usable format for TensorFlow\n",
        "# Format labels for use with TensorFlow"
      ],
      "metadata": {
        "id": "0oFPcMlqwoy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small BERT - small_bert/bert_en_uncased_L-4_H-512_A-8"
      ],
      "metadata": {
        "id": "IkTxXkNXPRaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_small_bert = []\n",
        "for F in tqdm(total_range_test):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "  text_encoded = encoding(split,small_bert_preprocess_model,small_bert_model)\n",
        "  pooled = text_encoded['pooled_output']\n",
        "  for i in range(1,len(pooled)):\n",
        "    X_test_small_bert.append(pooled[i]-pooled[i-1])"
      ],
      "metadata": {
        "id": "un13RLQNIlyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npX_test_small_bert = np.array(X_test_small_bert)"
      ],
      "metadata": {
        "id": "cZfA9rHVb8zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npX_test_small_bert)"
      ],
      "metadata": {
        "id": "HK6T-7phG_mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = []\n",
        "for F in tqdm(total_range_test):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation/truth-problem-'+str(F)+'.json'\n",
        "  with open(url, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    y_test.extend(data['changes'])"
      ],
      "metadata": {
        "id": "dsQueRNPG_vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npy_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "T7e4pV_WHKcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npy_test)"
      ],
      "metadata": {
        "id": "P5VmJfVHGUdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALBERT"
      ],
      "metadata": {
        "id": "9U6Y3uZBRCgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_albert = []\n",
        "for F in tqdm(total_range_test):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "  text_encoded = encoding(split,albert_preprocess_model,albert_model)\n",
        "  pooled = text_encoded['pooled_output']\n",
        "  for i in range(1,len(pooled)):\n",
        "    X_test_albert.append(pooled[i]-pooled[i-1])"
      ],
      "metadata": {
        "id": "kohJY1KDRCqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npX_test_albert = np.array(X_test_albert)"
      ],
      "metadata": {
        "id": "YoUGu4URIo-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npX_test_albert)"
      ],
      "metadata": {
        "id": "moAHLhnlIqoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npy_test)"
      ],
      "metadata": {
        "id": "ci9oeOsOLguc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small Electra"
      ],
      "metadata": {
        "id": "9H6W-x4n0M8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_electra = []\n",
        "for F in tqdm(total_range_test):\n",
        "  url = '/content/drive/MyDrive/pan23-multi-author-analysis-dataset3-validation/problem-'+str(F)+'.txt'\n",
        "  split = get_split_text(url)\n",
        "  text_encoded = encoding(split,small_electra_preprocess_model,small_electra_model)\n",
        "  pooled = text_encoded['pooled_output']\n",
        "  for i in range(1,len(pooled)):\n",
        "    X_test_electra.append(pooled[i]-pooled[i-1])"
      ],
      "metadata": {
        "id": "9k-wcr2t0umS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npX_test_electra = np.array(X_test_electra)"
      ],
      "metadata": {
        "id": "9Ya2KiI4P96p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npX_test_electra)"
      ],
      "metadata": {
        "id": "fFlTgQqYP9-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(npy_test)"
      ],
      "metadata": {
        "id": "K6s87FYgP-Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier"
      ],
      "metadata": {
        "id": "_LbhFX4IP-FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set metrics to show while training\n",
        "# Specify neural network classifier infrastructure\n",
        "# Train network\n",
        "# Perform prediction and provide summary of results"
      ],
      "metadata": {
        "id": "CF-uzk9Ywx6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall')\n",
        "]"
      ],
      "metadata": {
        "id": "UXbFhqGFR1jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small BERT"
      ],
      "metadata": {
        "id": "AAWwGPiNSAr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_small_bert = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dropout(0.1, name=\"dropout\"),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")\n",
        "])"
      ],
      "metadata": {
        "id": "FSSr4jL0R5Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small_bert.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "sXy9v8UoR5Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_small_bert = model_small_bert.fit(npX_train_small_bert, npy_train, epochs=15)"
      ],
      "metadata": {
        "id": "FwINplljR5cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_bert = model_small_bert.predict(npX_test_small_bert)\n",
        "y_pred_bert = y_pred_bert.flatten()\n",
        "y_pred_bert = np.where(y_pred_bert > 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "yW3oi-J-R5hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_small_bert = confusion_matrix(y_test, y_pred_bert)"
      ],
      "metadata": {
        "id": "MxM3bsMXTH07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.heatmap(cm_small_bert, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "tnOAExMYTH_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_bert))"
      ],
      "metadata": {
        "id": "wBus_c8ZTIIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b9911a-0c38-4e01-c0c7-862dd74b056b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.68      0.60      2159\n",
            "           1       0.50      0.35      0.41      1953\n",
            "\n",
            "    accuracy                           0.52      4112\n",
            "   macro avg       0.52      0.51      0.50      4112\n",
            "weighted avg       0.52      0.52      0.51      4112\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Albert"
      ],
      "metadata": {
        "id": "LmK-Ha8JP-Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_albert = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dropout(0.1, name=\"dropout\"),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")\n",
        "])"
      ],
      "metadata": {
        "id": "ceol1UZZP-MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_albert.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "2aVYMfKFP-Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_albert = model_albert.fit(npX_train_albert, npy_train, epochs=15)"
      ],
      "metadata": {
        "id": "i_0XKndjP-S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_albert = model_albert.predict(npX_test_albert)\n",
        "y_pred_albert = y_pred_albert.flatten()\n",
        "y_pred_albert = np.where(y_pred_albert > 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "6GiYOuCKP-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_albert = confusion_matrix(y_test, y_pred_albert)"
      ],
      "metadata": {
        "id": "5nI5qYthTvIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.heatmap(cm_albert, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "Sm4K1kmrTvNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_albert))"
      ],
      "metadata": {
        "id": "zhD1FxyZ7otd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a42d3d5-1d3d-4759-d69f-711b4e9e2d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.70      0.62      2159\n",
            "           1       0.53      0.38      0.44      1953\n",
            "\n",
            "    accuracy                           0.55      4112\n",
            "   macro avg       0.54      0.54      0.53      4112\n",
            "weighted avg       0.54      0.55      0.53      4112\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Electra"
      ],
      "metadata": {
        "id": "93YZcgnVP-Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_electra = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dropout(0.1, name=\"dropout\"),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")\n",
        "])"
      ],
      "metadata": {
        "id": "h5tZTIC3SeOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_electra.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "Sl0i4G8oSeSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_electra = model_electra.fit(npX_train_electra, npy_train, epochs=15)"
      ],
      "metadata": {
        "id": "S3PIDy48SeVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_electra = model_electra.predict(npX_test_electra)\n",
        "y_pred_electra = y_pred_electra.flatten()\n",
        "y_pred_electra = np.where(y_pred_electra > 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "7F_SYe_2SeY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_electra = confusion_matrix(y_test, y_pred_electra)"
      ],
      "metadata": {
        "id": "TNzjgodYSecI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn.heatmap(cm_electra, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "LvYRDjOGSefJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_electra))"
      ],
      "metadata": {
        "id": "nhTlcuxQSeij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103ccd68-f31e-458c-ea28-b1305b63e3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.75      0.63      2159\n",
            "           1       0.52      0.30      0.38      1953\n",
            "\n",
            "    accuracy                           0.53      4112\n",
            "   macro avg       0.53      0.52      0.50      4112\n",
            "weighted avg       0.53      0.53      0.51      4112\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpKQDa5kP-dV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
